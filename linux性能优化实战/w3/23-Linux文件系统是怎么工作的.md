## 23 | 基础篇：Linux 文件系统是怎么工作的？

### 索引节点和目录项

Linux文件系统为每个文件都分配两个数据结构，索引节点(index node)和目录项(directory entry)，他们主要用来记录文件的元信息和目录结构
+ 索引节点：简称inode，用来记录文件的元数据，比如inode编号，文件大小，访问权限，修改日期，数据的位置等，索引节点和文件一一对应，它跟文件内容一样都会被持久化存储到磁盘中，所以索引节点同样占用磁盘空间
+ 目录项：简称为dentry，用来记录文件的名字，索引节点指针以及与其他目录项的关联关系，多个关联的目录项，就构成了文件系统的目录结构，目录项是内核维护的一个内存数据结构，所以通常也被叫做目录项缓存

索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构，目录项和索引节点的关系是多对一，一个文件可以有多个别名

通过硬链接为文件创建的别名，会对应不同的目录项，但本质上还是连接同一个文件，所以他们的索引节点相同

磁盘读写的最小单位是扇区，扇区只有512B大小。文件系统把连续的扇区组成了逻辑块，每次都以逻辑块为最小单元，来管理数据，常见的逻辑块大小为4KB，由连续的8个扇区组成

其中目录项，索引节点以及文件数据的关系，可通过图片查看

其中有两点注意：
+ 目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据
+ 磁盘在执行文件系统格式化时，被分为三个存储区域，超级块，索引节点和数据块区，其中
	+ 超级块：存储整个文件系统的状态
	+ 索引节点区：存储索引节点
	+ 数据块区：存储文件数据

### 虚拟文件系统

Linux文件系统四大基本要素：目录项，索引节点，逻辑块，超级块

为了支持不同的文件系统，linux内核在用户进程和文件系统中间，又引入了一个抽象层，也就是虚拟文件系统VFS
查看linux文件系统架构图

文件系统按照存储位置不同分为三类：
+ 基于磁盘的文件系统，常见的Ext4、XFS，OverlayFS
+ 基于内存的文件系统，我们常说的虚拟文件系统，他们不需要任何磁盘分配存储空间，但会占用内存，比如常用的/proc文件系统，还有/sys文件系统
+ 网络文件系统，用来访问其他计算机数据的文件系统，比如NFS，SMB，ISCSI等

这些文件系统，要先挂载到VFS目录树种的某个子目录，才能访问其中的文件

### 文件系统I/O

文件读写方式的各种差异，导致I/O的分类多种多样，常见的又，缓冲与费缓冲I/O，直接与非直接I/O，阻塞与非阻塞I/O，同步与异步I/O等。

#### 是否利用标准库：缓冲与非缓冲

+ 缓冲I/O，利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件
+ 非缓冲I/O，直接通过系统调用来访问文件，不再经过标准库缓存

这里说的缓冲，是指标准库内部实现的缓存

他们最终都是要经过系统调用来访问文件。

#### 是否利用操作系统的页缓存：直接与非直接

+ 直接I/O，跳过操作系统的页缓存，直接跟文件系统交互访问文件
+ 非直接I/O，文件读写时，再经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘

如果要直接I/O，需要在系统调用中指定O_DIRECT标志。默认是非直接I/O

他们本质上还是和文件系统交互，

#### 应用程序是否阻塞自身运行：阻塞和非阻塞

+ 阻塞I/O，在程序执行I/O操作后，如没有获得响应，就会阻塞当前线程
+ 非阻塞I/O，不会阻塞线程，可以继续执行其他的任务，之后再通过轮询或者事件通知的形式，获取调用结果

#### 是否等待响应结果：同步和异步

+ 同步I/O，应用程序执行I/O操作后，要等到整个I/O完成后，才能获得I/O响应
+ 异步I/O，应用程序执行I/O操作后，不用等待完成和完成后的响应，而是继续执行就可以，响应会用事件通知的方式，告诉程序

举例说明：在操作文件时，如果设置了O_DSYNC或者O_DSYNC，就代表同步I/O，如果设置了O_DSYNC就要等待文件数据写入磁盘后，才能返回；而O_SYNC则是在O_DSYNC的基础上，要求文件元数据也要写入磁盘后才能返回；在访问管道或者网络套接字时，设置了O_ASYNC，相应的就是异步I/O，内核会通过SIGIO或者SIGPOLL，来通知进程文件是否读写

### 容量

一般通过df命令查看

问题1：磁盘空间不足，但通过df发现磁盘空间还有很多剩余空间，一般是小文件占用了过多了索引节点导致，删除小文件，或者移到其他磁盘中，就ok

#### 如何查看所有目录项和各种文件系统索引节点的缓存情况

```sh
cat /proc/slabinfo | grep -E '^#|dentry|inode'
# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail>
rpc_inode_cache       25     25    640   25    4 : tunables    0    0    0 : slabdata      1      1      0
xfs_inode         642510 642510   1088   30    8 : tunables    0    0    0 : slabdata  21417  21417      0
mqueue_inode_cache    288    288    896   36    8 : tunables    0    0    0 : slabdata      8      8      0
hugetlbfs_inode_cache     78     78    608   26    4 : tunables    0    0    0 : slabdata      3      3      0
sock_inode_cache    1428   1450    640   25    4 : tunables    0    0    0 : slabdata     58     58      0
shmem_inode_cache   2329   2400    680   24    4 : tunables    0    0    0 : slabdata    100    100      0
proc_inode_cache  152866 153336    656   24    4 : tunables    0    0    0 : slabdata   6389   6389      0
inode_cache        20682  20682    592   27    4 : tunables    0    0    0 : slabdata    766    766      0
dentry            828065 828345    192   21    1 : tunables    0    0    0 : slabdata  39445  39445      0
selinux_inode_security  10761  11322     80   51    1 : tunables    0    0    0 : slabdata    222    222      0
```

+ dentry:表示目录项缓存
+ inode_cache:表示VFS所以呢节点缓存
+ 其他的则是各种文件系统的索引节点缓存


也可以通过slabtop命令

```sh
$ slabtop
Last login: Tue Jun 16 17:22:38 2020 from 10.32.168.117
 Active / Total Objects (% used)    : 2252693 / 2294799 (98.2%)
 Active / Total Slabs (% used)      : 82730 / 82730 (100.0%)
 Active / Total Caches (% used)     : 68 / 94 (72.3%)
 Active / Total Size (% used)       : 1045359.93K / 1051769.12K (99.4%)
 Minimum / Average / Maximum Object : 0.01K / 0.46K / 8.00K

  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME
828324 828324 100%    0.19K  39444	 21    157776K dentry
642510 642510 100%    1.06K  21417	 30    685344K xfs_inode
319424 318961  99%    0.06K   4991	 64     19964K kmalloc-64
153216 152653  99%    0.64K   6384	 24    102144K proc_inode_cache
 56238  21241  37%    0.10K   1442	 39	 5768K buffer_head
```


