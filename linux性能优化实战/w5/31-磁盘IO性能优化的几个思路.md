## 31 | 套路篇：磁盘 I/O 性能优化的几个思路

### 如何知道磁盘I/O的瓶颈是多少

fio命令是测试文件系统和磁盘I/O性能基准测试工具，用来测试，裸盘或者文件系统在各种场景下的I/O性能，包括了不同块到校，不同I/O引擎以及是否使用缓存等场景

如下为测试命令

```sh

# 随机读
fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 随机写
fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 顺序读
fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb

# 顺序写
fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb 
```

解释：
+ direct:是否跳过系统缓存，1跳过
+ iodepth：使用异步I/O，同时发出的I/O情趣上限，上例中为64
+ rw：表示I/O模式，示例中为 read/write表示顺序读/写，而randread/randwrite表示随机读/写
+ ioengine：I/O引擎，支持同步（sync），异步（libaio）、内存映射（mmap）、网络（net）等各种
+ bs：表示I/O大小，设置为4k（也是默认值）
+ filename：文件路径，也可以是磁盘路径，

报告解析

```sh

read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.1
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=16.7MiB/s,w=0KiB/s][r=4280,w=0 IOPS][eta 00m:00s]
read: (groupid=0, jobs=1): err= 0: pid=17966: Sun Dec 30 08:31:48 2018
   read: IOPS=4257, BW=16.6MiB/s (17.4MB/s)(1024MiB/61568msec)
    slat (usec): min=2, max=2566, avg= 4.29, stdev=21.76
    clat (usec): min=228, max=407360, avg=15024.30, stdev=20524.39
     lat (usec): min=243, max=407363, avg=15029.12, stdev=20524.26
    clat percentiles (usec):
     |  1.00th=[   498],  5.00th=[  1020], 10.00th=[  1319], 20.00th=[  1713],
     | 30.00th=[  1991], 40.00th=[  2212], 50.00th=[  2540], 60.00th=[  2933],
     | 70.00th=[  5407], 80.00th=[ 44303], 90.00th=[ 45351], 95.00th=[ 45876],
     | 99.00th=[ 46924], 99.50th=[ 46924], 99.90th=[ 48497], 99.95th=[ 49021],
     | 99.99th=[404751]
   bw (  KiB/s): min= 8208, max=18832, per=99.85%, avg=17005.35, stdev=998.94, samples=123
   iops        : min= 2052, max= 4708, avg=4251.30, stdev=249.74, samples=123
  lat (usec)   : 250=0.01%, 500=1.03%, 750=1.69%, 1000=2.07%
  lat (msec)   : 2=25.64%, 4=37.58%, 10=2.08%, 20=0.02%, 50=29.86%
  lat (msec)   : 100=0.01%, 500=0.02%
  cpu          : usr=1.02%, sys=2.97%, ctx=33312, majf=0, minf=75
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwt: total=262144,0,0, short=0,0,0, dropped=0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64

Run status group 0 (all jobs):
   READ: bw=16.6MiB/s (17.4MB/s), 16.6MiB/s-16.6MiB/s (17.4MB/s-17.4MB/s), io=1024MiB (1074MB), run=61568-61568msec

Disk stats (read/write):
  sdb: ios=261897/0, merge=0/0, ticks=3912108/0, in_queue=3474336, util=90.09% 
```

+ slat:从I/O提交到实际执行I/O时长
+ clat：从I/O提交到I/O完成的时长
+ lat：从fio创建I/O到I/O完成的总时长

对于同步I/O来说，由于I/O提交和I/O完成是一个动作，所以slat实际上就是I/O完成的时间，而clat是0。使用异步I/O时候，lat近似等于slat+clat之和

fio支持I/O重放，借助blktrace。

```sh

# 使用blktrace跟踪磁盘I/O，注意指定应用程序正在操作的磁盘
$ blktrace /dev/sdb

# 查看blktrace记录的结果
# ls
sdb.blktrace.0  sdb.blktrace.1

# 将结果转化为二进制文件
$ blkparse sdb -d sdb.bin

# 使用fio重放日志
$ fio --name=replay --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin 
```

### 性能优化

#### 应用程序优化

+ 用追加写代替随机写，减少寻址开销，加快I/O写的速度
+ 借助缓存I/O，充分利用系统缓存，降低实际I/O的次数
+ 在应用程序内部构件自己的缓存，或者用redis这类外部缓存系统。
+ 在需要频繁读写同一块磁盘空间时，用那个mmap代替read/write，减少内存的拷贝次数
+ 在需要同步写的场景中，尽量将写请求合并，即fsync（）取代O_SYNC
+ 在多个应用程序共享相同磁盘时，为了抱枕I/O不被某个应用完全占用，使用cgroups的I/O子系统，来限制进程/进程组的IOPS以及吞吐量
+ 在使用CFQ调度器时，用ionice来调整进程的I/O调度优先级，提交和兴应用的I/O优先级，它支持三个优先级类，idle、Best-effort和Readltime。其中Best-effort和Realtime还分别支持0-7的级别，数值越小，表示优先级别越高

#### 文件系统优化

+ 根据手机负载场景不同，选择最适合的文件系统，例如：Ubuntu使用ext4，CentOS7使用xfs
+ 优化文件系统的配置选项，包括文件系统的特性如（ext_attr、dir_index）、日志模式（如journal、ordered、writeback）、挂载选项（noatime）等等
+ 优化文件系统的缓存，
	+ 优化pdflush脏页的刷新频率（比如设置 dirty_expire_centisecs 和 dirty_writeback_centisecs）以及脏页的限额（比如调整 dirty_background_ratio 和 dirty_ratio 等；
	+ 还可以优化内核回收目录项缓存和索引节点缓存的倾向，即调整 vfs_cache_pressure（/proc/sys/vm/vfs_cache_pressure，默认值 100），数值越大，就表示越容易回收；
	+ 在不需要持久化时，还可以用内存文件系统tmpfs，以获得更好的I/O性能，他把数据直接保存在内存中，默认是总内存的一半

#### 磁盘优化

+ 通过SSD替换HDD
+ 使用RAID
+ 根据磁盘和应用程序I/O模式的特征，选择最合适的I/O的调度算法，比如SSD和虚拟机中的磁盘，通常采用noop调度算法，而数据库应用更推荐使用deadline算法
+ 对应用程序的数据，进行磁盘级别的隔离，比如，为日志、数据库等I/O压力比较中的应用单独配置磁盘
+ 在顺序读比较多的场景中，增大磁盘的预读数据，通过下面两种方法可做到
	+ 调整内核选项 /sys/block/sdb/queue/read_ahead_kb，默认大小是 128 KB，单位为 KB
	+ 使用 blockdev 工具设置，比如 blockdev --setra 8192 /dev/sdb，注意这里的单位是 512B（0.5KB），所以它的数值总是 read_ahead_kb 的两倍
+ 优化内核块设备I/O的选项，比如，调整磁盘队列的长度/sys/block/sdb/queue/nr_requests，适当增大队列长度，提升磁盘的吞吐量，不过不导致I/O延迟增大
+ 通过dmesg检查硬件I/O故障日志，还可使用badblocks、smartctl等工具，检测磁盘的硬件问题，或使用e2fsck检测文件系统的错误。发现问题使用fsck工具来修复

