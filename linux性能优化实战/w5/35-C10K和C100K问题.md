## 35 | 基础篇：C10K 和 C1000K 回顾

### C10K问题

单机同时处理1玩个请求

#### I/O模型优化

使用异步、非阻塞I/O的解决思路。即I/O多路复用

两种I/O事件通知的方式：
+ 水平触发：只要文件描述符可以非阻塞的执行I/O，就会出发通知，也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行操作
+ 边缘触发：只有文件描述符的状态发生改变，才发送一次通知，这时程序需要尽可能多的执行I/O，直到无法继续读写，才可以停止，如果I/O没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了

#### 实现I/O多路复用的方法

##### 使用非阻塞I/O和水平触发通知，使用select或者poll，
由于I/O是非阻塞的，一个线程就可以同时监控一批套接字的文件描述符，
+ 优点：对应用程序比较友好，而且API非常简单。
+ 缺点：当请求数量多的时候，轮训就会比较耗时。
	+ select使用固定长度的位相量，表示文件描述符的集合，因此会有最大描述符数量的限制，在32位系统中，默认限制是1024，
	+ 并且select内部，检查套接字状态时轮训的方法，再加上应用软件使用时的轮训，就变成的O(n^2)的关系了，
	+ poll改进了select的方法，换成了不固定长度的数组，没有了最大描述符的限制，但应用程序使用poll时候，同样需要对文件描述符列表进行轮训
	+ 应用程序每次调用select和poll时，还需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间，增加了处理成本

##### 使用非阻塞I/O和边缘触发通知，epoll

特点：epoll使用红黑树，在内核管理文件描述符的集合，这样，就不需要应用程序在每次操作时传入、传出集合；使用事件驱动的机制，只关注有I/O事件发生的文件描述符，不需要轮训扫描整个集合

##### 使用异步 I/O（Asynchronous I/O，简称为 AIO）

使用难度比较高

#### 工作模型优化

##### 主进程+多个worker子进程

工作模式：
+ 主进程执行bind（）+listen（）后，创建多个子进程
+ 每个子进程中通过accept（）或epoll_wait()来处理相同的套接字

惊群问题：当accept（）或epoll_wait()调用时候，多个进程被同时唤醒，但实际只有一个进程来响应此次事件，其他的被唤醒的又会重新休眠

惊群问题 accept（）在linux2.6中解决，而epoll的问题，则子linux4.5才通过EPOLLEXCLUSIVE解决

nginx通过为每个worker进程中，增加一个全局锁（accept_mutex），这些worker进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到epoll中，确保了只有一个子进程被唤醒

nginx性能好的原因：它的这些worker进程，并不会经常的创建和销毁，在没任务时候回休眠，有任务唤醒。只有某些异常退出时，才会重新创建新的worker子进程。可以使用线程替换进程的方式

##### 监听到相同端口的多进程模型

所有进程监听相同的接口，并且开启了SO_REUSEPORT选项，由内核负责将请求负载均衡到这些监听进程中去。内核确保了只有一个进程被唤醒，不会出现惊群问题。这个需要在linux3.9以上版本才可行

### C1000K问题

本质上还是构建在epoll的非阻塞I/O模型之上，只不过除了I/O模型之外还要从应用程序到内核，再到cpu以及内存，网络等各个层次的深度优化。

### C10M问题

本质是：跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序那里去，常见的机制有DPDK和XDP

+ DPDK：用户态网络的标准，它跳过内核协议栈，直接由用户态进程通过轮训的方式，来处理网络接收
	+ 在PPS非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包
	+ 跳过内核协议栈后，省去了繁杂的硬中断，软中断，再到linux网络协议栈逐层处理的过程，应用程序可以针对应用的实际场景，有针对性的优化网络包的处理逻辑
	+ 通过大页，CPU绑定，内存对齐，流水线并发等多种机制，优化网络包的处理效率
+ XDP（eXpress Data Path）：由linux内核提供的高性能网络数据路径，它允许网络包，在进入内核协议栈之前，就进行处理，也可以带来更高的性能。对内核要求linux4.8以上版本。并且不提供缓存队列。基于XDP的应用程序通常是专用的网络应用，常见的又IDS，DDoS防御

，cilium容器网络插件。


