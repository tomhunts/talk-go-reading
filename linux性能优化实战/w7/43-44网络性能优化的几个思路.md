## 43-44 网络性能优化的几个思路

### 确定目标

网络性能优化的整体目标，是降低网络延迟，提高吞吐量，但具体到不同的应用中，每个指标的优化标准可能会不同

比如：对NAT网关来说：PPS是最主要的性能目标
	对数据库、缓存等系统：哭诉完成网络收发，低延迟是主要性能目标

按照网络协议栈来进行区别对待

网络接口层和网络层：主要负责网络包的封装、寻址、路由、以及发送和接收，每秒可处理的网络包数PPS，他们是最重要的性能指标，通过pktgen来测试PPS性能
传输层TCP和UDP：他们主要负责网络传输，吞吐量BPS、连接数以及延迟。是最主要的性能指标，可使用iperf，和netperf来测试，注意网络包的大小也会影响指标的值
应用层：关注吞吐量BPS、每秒请求数以及延迟等指标，可使用wrk、ab等工具

### 性能与工具

主要看一下本目录下的工具与性能的两个关系图进行学习

### 网络性能优化

下面从几个角度来说明相应优化的基本思路

#### 应用程序
通常通过套接字接口进行网络操作，主要是对网络I/O和进程自身的工作模型的优化

从网络I/O来说：
+ I/O多路复用技术epoll
+ 使用异步I/O即AIO，但这个相对来说比较复杂。

从工作模型来说：
+ 主进程+多个worker子进程的方式，主进程负责管理网络连接，子进程负责实际的业务处理。
+ 监听到相同端口的多进程模型，这种模型下，所有进程都会监听相同端口，并且开启SO_REUSEPORT选项，由内核把请求负载均衡到相应的进程中去

从应用层的网路协议优化：
+ 使用长连接取代短连接，在每秒请求次数较多时，这样做效果非常明显
+ 使用内存等方式来缓存不常变化的数据
+ 使用Protocol Buffer等序列化的方式，压缩网络I/O的数据量
+ 使用DNS缓存、预取、HTTPDNS等方式酱烧DNS解析的延迟。

#### 套接字

套接字可以屏蔽掉Linux内核中不同协议的差异，为应用程序提供统一的访问接口，每个套接字，都有一个读写缓冲区。
读缓冲区：缓存远端发过来的数据，如果缓冲区满，则不能接受新的数据
写缓冲区：缓存要发过去的数据，如果写缓冲区满，应用程序的写操作会被阻塞

因此为了提高网络吞吐量，通常要调整这些缓冲区的大小：
+ 增大每个套接字的缓冲区大小net.core.optmem_max  参考值：81920
+ 增大套接字接收缓冲区大小 net.core.rmem_max 和发送缓冲区大小 net.core.wmem_max  参考值：513920
+ 增大 TCP 接收缓冲区大小 net.ipv4.tcp_rmem 和发送缓冲区大小 net.ipv4.tcp_wmem  参考值：4096  87380  16777216
+ 增大UDP缓冲区范围 net.ipv4.udp_mem  参考值：188562  251418  377124

注意：上面的tcp_rmem和tcp_wmem的三个参考值是min，default，max。系统会根据这些设置，自动调整TCP接收/发送缓冲区的大小；
udp_mem的三个值分别是min,pressure,max。同样系统自动调节大小

除以上之外，套接字接口提供了一些配置选项，用来修改网络连接的行为：
+ 为 TCP 连接设置 TCP_NODELAY 后，就可以禁用 Nagle 算法；
+ 为 TCP 连接开启 TCP_CORK 后，可以让小包聚合成大包后再发送（注意会阻塞小包的发送）
+ 使用 SO_SNDBUF 和 SO_RCVBUF ，可以分别调整套接字发送缓冲区和接收缓冲区的大小

#### 传输层

主要针对TCP和UDP两个协议的优化

##### TCP优化

第一类：在请求数比较大的场景下，会看到大量处于TIME_WAIT状态的链接，可采用如下几种措施

+ 增大处于TIME_WAIT状态的链接数量net.ipv4.tcp_max_tw_buckets，并增大链接跟踪表的大小net.netfilter.nf_conntrack_max
+ 减小 net.ipv4.tcp_fin_timeout 和 net.netfilter.nf_conntrack_tcp_timeout_time_wait ，让系统尽快释放它们所占用的资源
+ 开启端口复用 net.ipv4.tcp_tw_reuse。这样，被 TIME_WAIT 状态占用的端口，还能用到新建的连接中
+ 增大本地端口的范围 net.ipv4.ip_local_port_range 。这样就可以支持更多连接，提高整体的并发能力
+ 增加最大文件描述符的数量。你可以使用 fs.nr_open 和 fs.file-max ，分别增大进程和系统的最大文件描述符数；或在应用程序的 systemd 配置文件中，配置 LimitNOFILE ，设置应用程序的最大文件描述符数

第二类：为了缓解 SYN FLOOD 等，利用 TCP 协议特点进行攻击而引发的性能问题，优化与SYN状态相关的内核选项

+ 增大 TCP 半连接的最大数量 net.ipv4.tcp_max_syn_backlog ，或者开启 TCP SYN Cookies net.ipv4.tcp_syncookies ，来绕开半连接数量限制的问题（注意，这两个选项不可同时使用）
+ 减少 SYN_RECV 状态的连接重传 SYN+ACK 包的次数 net.ipv4.tcp_synack_retries

第三类：优化长连接中使用Keepalive 来检测 TCP 连接的状态的相关内核选项
+ 缩短最后一次数据包到 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_time
+ 缩短发送 Keepalive 探测包的间隔时间 net.ipv4.tcp_keepalive_intvl
+ 减少 Keepalive 探测失败后，一直到通知应用程序前的重试次数 net.ipv4.tcp_keepalive_probes


##### UDP优化

+ 增大套接字缓冲区大小以及 UDP 缓冲区范围
+ 增大本地端口号的范围
+ 根据MTU大小，调整UDP数据包的大小，减少或者避免分片的发生

#### 网络层

对路由、IP分片以及ICMP等调优

##### 从路由和转发的角度出发

+ 在需要转发的服务器中，比如用作 NAT 网关的服务器或者使用 Docker 容器时，开启 IP 转发，即设置 net.ipv4.ip_forward = 1
+ 调整数据包的生存周期 TTL，比如设置 net.ipv4.ip_default_ttl = 64。注意，增大该值会降低系统性能。
+ 开启数据包的反向地址校验，比如设置 net.ipv4.conf.eth0.rp_filter = 1。这样可以防止 IP 欺骗，并减少伪造 IP 带来的 DDoS 问题

##### 从分片的角度出发，主要调整MTU的大小
MTU的大小应用根据以太网的标准来设置，以太网规定，一个网络帧最大为1518B，去掉头部的18B后，剩余的1500就是MTU的大小
针对不同的叠加网络技术，调整相应的MTU大小

##### 从ICMP角度出发
+ 禁止ICMP协议：net.ipv4.icmp_echo_ignore_all = 1
+ 禁止广播ICMP：net.ipv4.icmp_echo_ignore_broadcasts = 1

#### 链路层

+ 为网卡硬中断配置CPU亲和性（smp_affinity），或者开启irqbalance服务
+ 开启RPS（Receive Packet Steering）和 RFS（Receive Flow Steering），将应用程序和软中断的处理，调度到相同的CPU上，这样可增大CPU缓存命中率，减少网络延迟

网卡功能进行了增强
+ TSO（TCP Segmentation Offload）和 UFO（UDP Fragmentation Offload）：在 TCP/UDP 协议中直接发送大包；而 TCP 包的分段（按照 MSS 分段）和 UDP 的分片（按照 MTU 分片）功能，由网卡来完成
+ GSO（Generic Segmentation Offload）：在网卡不支持 TSO/UFO 时，将 TCP/UDP 包的分段，延迟到进入网卡前再执行。这样，不仅可以减少 CPU 的消耗，还可以在发生丢包时只重传分段后的包
+ LRO（Large Receive Offload）：在接收 TCP 分段包时，由网卡将其组装合并后，再交给上层网络处理。不过要注意，在需要 IP 转发的情况下，不能开启 LRO，因为如果多个包的头部信息不一致，LRO 合并会导致网络包的校验错误。
+ GRO（Generic Receive Offload）：GRO 修复了 LRO 的缺陷，并且更为通用，同时支持 TCP 和 UDP
+ RSS（Receive Side Scaling）：也称为多队列接收，它基于硬件的多个接收队列，来分配网络接收进程，这样可以让多个 CPU 来处理接收到的网络包
+ VXLAN 卸载：也就是让网卡来完成 VXLAN 的组包功能

针对网络接口本身的优化
+ 开启网络接口的多队列功能
+ 增大网络接口的缓冲区大小，以及队列长度。
+ 使用Traffic Control 工具，为不同网络流量配置 QoS

### DPDK与XDP技术

+ 使用 DPDK 技术，跳过内核协议栈，直接由用户态进程用轮询的方式，来处理网络请求。同时，再结合大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率
+ 使用内核自带的 XDP 技术，在网络包进入内核协议栈前，就对其进行处理，这样也可以实现很好的性能。











