## 46-52 综合实战案例篇

### 案例1：应用容器化后，启动慢了很多

案例准备：tomcat应用 在容器中启动，通过curl访问tomcat应用时候返回“Connection reset by peer”。

+ 首先通过应用日志信息查看是否有错误信息
+ 通过容器状态发现是被OOM所杀死，OOM是系统在应对内存不足时，杀死相应的程序。
+ 之后通过dmesg 命令查看OOM日志，

```sh

$ dmesg
[193038.106393] java invoked oom-killer: gfp_mask=0x14000c0(GFP_KERNEL), nodemask=(null), order=0, oom_score_adj=0
[193038.106396] java cpuset=0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53 mems_allowed=0
[193038.106402] CPU: 0 PID: 27424 Comm: java Tainted: G  OE    4.15.0-1037 #39-Ubuntu
[193038.106404] Hardware name: Microsoft Corporation Virtual Machine/Virtual Machine, BIOS 090007  06/02/2017
[193038.106405] Call Trace:
[193038.106414]  dump_stack+0x63/0x89
[193038.106419]  dump_header+0x71/0x285
[193038.106422]  oom_kill_process+0x220/0x440
[193038.106424]  out_of_memory+0x2d1/0x4f0
[193038.106429]  mem_cgroup_out_of_memory+0x4b/0x80
[193038.106432]  mem_cgroup_oom_synchronize+0x2e8/0x320
[193038.106435]  ? mem_cgroup_css_online+0x40/0x40
[193038.106437]  pagefault_out_of_memory+0x36/0x7b
[193038.106443]  mm_fault_error+0x90/0x180
[193038.106445]  __do_page_fault+0x4a5/0x4d0
[193038.106448]  do_page_fault+0x2e/0xe0
[193038.106454]  ? page_fault+0x2f/0x50
[193038.106456]  page_fault+0x45/0x50
[193038.106459] RIP: 0033:0x7fa053e5a20d
[193038.106460] RSP: 002b:00007fa0060159e8 EFLAGS: 00010206
[193038.106462] RAX: 0000000000000000 RBX: 00007fa04c4b3000 RCX: 0000000009187440
[193038.106463] RDX: 00000000943aa440 RSI: 0000000000000000 RDI: 000000009b223000
[193038.106464] RBP: 00007fa006015a60 R08: 0000000002000002 R09: 00007fa053d0a8a1
[193038.106465] R10: 00007fa04c018b80 R11: 0000000000000206 R12: 0000000100000768
[193038.106466] R13: 00007fa04c4b3000 R14: 0000000100000768 R15: 0000000010000000
[193038.106468] Task in /docker/0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53 killed as a result of limit of /docker/0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53
[193038.106478] memory: usage 524288kB, limit 524288kB, failcnt 77
[193038.106480] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0
[193038.106481] kmem: usage 3708kB, limit 9007199254740988kB, failcnt 0
[193038.106481] Memory cgroup stats for /docker/0f2b3fcdd2578165ea77266cdc7b1ad43e75877b0ac1889ecda30a78cb78bd53: cache:0KB rss:520580KB rss_huge:450560KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:520580KB inactive_file:0KB active_file:0KB unevictable:0KB
[193038.106494] [ pid ]   uid  tgid total_vm      rss pgtables_bytes swapents oom_score_adj name
[193038.106571] [27281]     0 27281  1153302   134371  1466368        0             0 java
[193038.106574] Memory cgroup out of memory: Kill process 27281 (java) score 1027 or sacrifice child
[193038.148334] Killed process 27281 (java) total-vm:4613208kB, anon-rss:517316kB, file-rss:20168kB, shmem-rss:0kB
[193039.607503] oom_reaper: reaped process 27281 (java), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB
```
分析：
	+ 被杀死的是个java进程，从内核调用栈上的mem_cgroup_out_of_memory可看出，超过了cgroup的内存限制，被OOM杀死
	+ jajva进程在容器内运行，容器的内存使用率和限制都是512M（524288kB），超过了限制量，导致OOM
	+ PID为27281，虚拟内存为4.3G（total-vm:4613208kb）,匿名内存为505M（anon-rss:517316kB）,也内存为19M（20168kB），匿名内存时主要的内存占用，而且匿名内存加上页内存，总共是524M，已超过512M限制

+ 综上分析，可看出匿名内存占用过多，导致OOM杀死了进程，而匿名内存就是主动分配申请的堆内存。
+ 通过程序调用影响的命令查看java所配置的堆内存大小，发现没有限制应用的堆内存大小
+ 通过命令配置java相应的环境变量。

### 案例2：服务器总是间接性的丢包

案例准备：通过hping3 测试nginx服务器发现有50%的包丢失。

丢包问题可能出现在任何一个位置，从协议栈中逐步的分析

+ 链路层：通过ethtool或者netstat -i 查看网络的丢包情况。

```sh
root@app1401:/home# netstat  -i
Kernel Interface table
Iface      MTU    RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
eth0      1500 3662784047      0  56987 0      3171493191      0      0      0 BMRU
lo       65536 56985371      0      0 0      56985371      0      0      0 LRU
```
输出中的 RX-OK、RX-ERR、RX-DRP、RX-OVR ，分别表示接收时的总包数、总错误数、进入 Ring Buffer 后因其他原因（如内存不足）导致的丢包数以及 Ring Buffer 溢出导致的丢包数
TX-OK、TX-ERR、TX-DRP、TX-OVR 也代表类似的含义，只不过是指发送时对应的各个指标。

+ 如果配置了tc规则，也会导致丢包 通过tc -s qdisc show dev eth0 命令查看输出统计信息

```sh
root@nginx:/# tc -s qdisc show dev eth0
qdisc netem 800d: root refcnt 2 limit 1000 loss 30% 
Sent 432 bytes 8 pkt (dropped 4, overlimits 0 requeues 0) 
backlog 0b 0p requeues 0
```
可看到eth0上配置了一个网络模拟排队规则，并且配置了丢包率为30%。删除后 解决
+ 如果还没解决就要继续往上层排查
+ 网路层和传输层：通过netstat -s命令 查看协议的收发汇总情况。可以看到tcp协议有多次超时和失败重试，并且主要错误是半连接重置。

```sh

root@nginx:/# netstat -s
Ip:
    Forwarding: 1          //开启转发
    31 total packets received    //总收包数
    0 forwarded            //转发包数
    0 incoming packets discarded  //接收丢包数
    25 incoming packets delivered  //接收的数据包数
    15 requests sent out      //发出的数据包数
Icmp:
    0 ICMP messages received    //收到的ICMP包数
    0 input ICMP message failed    //收到ICMP失败数
    ICMP input histogram:
    0 ICMP messages sent      //ICMP发送数
    0 ICMP messages failed      //ICMP失败数
    ICMP output histogram:
Tcp:
    0 active connection openings  //主动连接数
    0 passive connection openings  //被动连接数
    11 failed connection attempts  //失败连接尝试数
    0 connection resets received  //接收的连接重置数
    0 connections established    //建立连接数
    25 segments received      //已接收报文数
    21 segments sent out      //已发送报文数
    4 segments retransmitted    //重传报文数
    0 bad segments received      //错误报文数
    0 resets sent          //发出的连接重置数
Udp:
    0 packets received
    ...
TcpExt:
    11 resets received for embryonic SYN_RECV sockets  //半连接重置数
    0 packet headers predicted
    TCPTimeouts: 7    //超时数
    TCPSynRetrans: 4  //SYN重传数
  ...
```
+ 网络iptables也会存在丢包问题，通过iptables -t filter -nvL 查看各个规则的统计信息
+ 端口正常，但是访问nginx服务却是超时，可用tcpdump抓包工具进行协议抓取
+ 通过hping3没有问题，但是curl有问题，因为hping3只发送了syn包，而curl发送完syn包后，还会发送get请求。
+ 此时思考应该是MTU配置错误导致。修改ifconfig eth0 mtu 1500 增大为1500 之后正常

### 案例3：内核线程CPU利用率太高

在linux中，用户态进程的祖先，都是PID号为1的init进程，init都是systemd进程，其他的用户态进程会通过systemd来进行管理
在linux启动过程中，有三个特殊的进程
	1.0号进程为idle进程，这是系统创建的第一个进程，他初始化1号和2号进程后，演变为空闲任务，当CPU没有其他任务执行时，运行
	2.1号进程为init进程，通常是systemd进程，在用户态运行，用来管理其他用户态进程
	3.2号进程为kthreadd进程，在内核态运行，用来管理内核线程
以下为常见的几个内核线程：
kswapd0：用户内存回收
kworker：用于执行内核工作队列，分为绑定 CPU （名称格式为 kworker/CPU86330）和未绑定 CPU（名称格式为 kworker/uPOOL86330）两类
migration：在负责均衡过程中，把进程迁移到CPU上，每个CPU都有一个migration内核线程
jbd2/sda1-8：jbd 是 Journaling Block Device 的缩写，用来为文件系统提供日志功能，以保证数据的完整性；名称中的 sda1-8，表示磁盘分区名称和设备号。每个使用了 ext4 文件系统的磁盘分区，都会有一个 jbd2 内核线程。
pdflush：用于将内存中的脏页（被修改过，但还未写入磁盘的文件页） 写入磁盘

+ 通过top命令查看软中断使用率超过30%，同时软中断内核线程使用了过多资源
+ 对于普通进程可以使用strace，pstack。lsof等命令，而对于内核线程使用perf命令可以分析，使用perf record -a -g -p 9 -- sleep 30 进行采样数据，之后使用perf report命令查看调用关系链图
+ 通过火焰图查看更直观的调用结果
+ 火焰图可以分为以下几类：
	+ on-CPU 火焰图：表示 CPU 的繁忙情况，用在 CPU 使用率比较高的场景中。
	+ off-CPU 火焰图：表示 CPU 等待 I/O、锁等各种资源的阻塞情况。
	+ 内存火焰图：表示内存的分配和释放情况。
	+ 热 / 冷火焰图：表示将 on-CPU 和 off-CPU 结合在一起综合展示。
	+ 差分火焰图：表示两个火焰图的差分情况，红色表示增长，蓝色表示衰减。差分火焰图常用来比较不同场景和不同时期的火焰图，以便分析系统变化前后对性能的影响情况。

+ 生成火焰图：
	$ git clone https://github.com/brendangregg/FlameGraph
	$ cd FlameGraph
	$ perf script -i /root/perf.data | ./stackcollapse-perf.pl --all |  ./flamegraph.pl > ksoftirqd.svg
	使用浏览器 打开ksoftirqd.svg即可看到火焰图

### 案例4：动态追踪怎么用

动态追踪技术：通过探针机制，来菜鸡内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题。

Dtrace：是动态追踪技术的鼻祖，它运行常驻在内核中，用户可以通过dtrace命令，把D语言编写的动态追踪脚本，提交到内核中的运行时来执行。它可以追踪用户态和内核态的所有事件，并通过一系列的措施，保证最小的性能开销。它无法在Linux中运行。
SystemTap:将Dtrace移植到linux上最著名的动态追踪框架。它并没有常驻内核的运行时，它需要先把脚本编译为内核模块，然后再插入到内核中执行。

#### 动态追踪的事件源

根据事件类型的不同：静态探针、动态探针以及硬件事件等三类

硬件事件：通常由性能监控计数器PMC产生，包括了各种硬件的性能情况，比如CPU的缓存、指令周期、分支预测等
静态探针：事先在代码中定义好，并编译到应用程序或内核中的探针。这些探针只有在开启探测功能时，才会被执行。常见的静态探针包括内核中的跟踪点和USDT探针
	+ 跟踪点（tracepoints）：实际上是就是在源码中插入的一些带有控制条件的探测点，这些探测点允许事后再添加处理函数，比如在内核中，最常见的静态跟踪方法就是printk，即输出日志。
	+ USDT探针：全称是用户级静态定义跟踪，需要在源码中插入DTRACE_PROBE()代码，并编译到应用程序中，不过，也有很多应用程序内置了USDT探针。
动态探针：没有事先在代码中定义，但却可以在运行时动态添加的探针。常见的动态探针有两种，即用于内核态的kprobes和用于用户态的uprobes
	+ kprobes：用来跟踪内核态的函数。需要内核编译时开启CONFIG_KPROBE_EVENTS
	+ uprobes：用来跟踪用户态的函数。需要内核编译时开启CONFIG_UPROBE_EVENTS

#### 动态追踪机制

ftrace：用于函数跟踪，后来又扩展支持了各种事件跟踪功能。通过debugfs，以普通文件的形式，向用户空间提供访问接口
perf：他是一种最简单的静态跟踪机制。也可以通过自定义胴体事件，来关注真正感兴趣的事件
eBPF：在BPF基础上扩展而来。不仅支持事件跟踪机制，还可以通过自定义的BPF代码来自由扩展。他常驻内核的运行时

除了以上这些之外还有SystemTap，BCC，sysdig等

下面通过例子展示相应的使用

##### ftrace

```sh
//切换到挂载点
cd /sys/kernel/debug/tracing

//如果目录未找到，则进行挂载
mount -t debugfs nodev /sys/kernel/debug

//查看所支持的跟踪器
cat available_tracers
最常用的就是function：函数跟踪执行，function_graph跟踪函数的调用关系。

//查看支持的函数
$ cat available_filter_functions

//查看支持的事件
$ cat available_events
```

使用步骤跟踪打开文件的ls命令函数：
1. 设置跟踪函数为打开文件：$ echo do_sys_open > set_graph_function
2. 配置跟踪选项，开启函数调用跟踪，并跟踪调用进程 
$ echo function_graph > current_tracer
$ echo funcgraph-proc > trace_options
3. 开启跟踪$ echo 1 > tracing_on
4. 执行ls命令后，在关闭跟踪$ echo 0 > tracing_on
5. 查看跟踪结果$ cat trace

```sh

$ cat trace
# tracer: function_graph
#
# CPU  TASK/PID         DURATION                  FUNCTION CALLS
# |     |    |           |   |                     |   |   |   |
 0)    ls-12276    |               |  do_sys_open() {
 0)    ls-12276    |               |    getname() {
 0)    ls-12276    |               |      getname_flags() {
 0)    ls-12276    |               |        kmem_cache_alloc() {
 0)    ls-12276    |               |          _cond_resched() {
 0)    ls-12276    |   0.049 us    |            rcu_all_qs();
 0)    ls-12276    |   0.791 us    |          }
 0)    ls-12276    |   0.041 us    |          should_failslab();
 0)    ls-12276    |   0.040 us    |          prefetch_freepointer();
 0)    ls-12276    |   0.039 us    |          memcg_kmem_put_cache();
 0)    ls-12276    |   2.895 us    |        }
 0)    ls-12276    |               |        __check_object_size() {
 0)    ls-12276    |   0.067 us    |          __virt_addr_valid();
 0)    ls-12276    |   0.044 us    |          __check_heap_object();
 0)    ls-12276    |   0.039 us    |          check_stack_object();
 0)    ls-12276    |   1.570 us    |        }
 0)    ls-12276    |   5.790 us    |      }
 0)    ls-12276    |   6.325 us    |    }
...
```

第一列表示运行CPU，第二列是任务名称和进程PID，第三列是函数执行延迟，最后一列是函数调用关系图。

如果感觉此5步骤麻烦，可使用trace-cmd命令进行简化，使用：trace-cmd record -p function_graph -g do_sys_open -O funcgraph-proc ls 进行配置； 使用trace-cmd report 进行查看结果


##### perf

案例一为ls命令
```sh
//查看所支持的事件
perf list

//添加探针
perf probe --add do_sys_open

//采集数据
perf record -e probe:do_sys_open -aR sleep 1

//查看
perf script
发现没有参数信息，不知道打开的文件路径是在哪

//查看do_sys_open所有的参数信息
perf probe -V do_sys_open
如果此命令失败说明调试符号表没有安装，安装调试信息后重试

//不可以重复添加探针，此次删除旧的探针
perf probe --del probe:do_sys_open

//添加带参数的探针
perf probe --add 'do_sys_open filename:string'
之后在查看就可以看到打开的文件路径了

```

strace能实现同样的功能但是相对来说需要更高的性能。strace基于系统调用ptrace实现
+ 由于ptrace是系统调用，需要在内核态和用户态切换，当事件数量比较多时，繁忙的切换必然会影响原有服务的性能
+ ptrace需要借助SIGSTOP信号挂起目标进程，这种信号控制和进程挂起，会影响目标进程的行为

所以在性能敏感的应用中，并不推荐使用strace。

案例二：为用户空间的库函数

```sh

# 为/bin/bash添加readline探针
$ perf probe -x /bin/bash 'readline%return +0($retval):string’

# 采样记录
$ perf record -e probe_bash:readline__return -aR sleep 5

# 查看结果
$ perf script
    bash 13348 [000] 93939.142576: probe_bash:readline__return: (5626ffac1610 <- 5626ffa46739) arg1="ls"

# 跟踪完成后删除探针
$ perf probe --del probe_bash:readline__return


# 查询所有的函数
$ perf probe -x /bin/bash —funcs

# 查询函数的参数
$ perf probe -x /bin/bash -V readline
Available variables at readline
        @<readline+0>
                char*   prompt
```

##### eBPF和BCC

eBPF执行需要三步：
	+ 从用户跟踪程序生成BPF字节码
	+ 加载到内核中运行
	+ 想用户空间输出结果
因为复杂，所以诞生了BCC，

BCC：把eBPF中的各种事件源，和数据操作，都转换为了python接口因为需要跟内核中的数据结构交互，真正核心的事件处理逻辑，还是需要我们用C语言编写。

BCC安装完成后会放在/usr/share/bcc/examples目录下

BCC动态跟踪步骤：
	+ 使用之前导入模块
	+ 定义事件以及处理事件的函数，需要用C语言编写。
	+ 定义一个输出函数，把输出函数跟BPF事件绑定
	+ 执行事件循环，开始追踪函数的调用。

### 案例5：服务器吞吐量下降的厉害怎么分析？

+ 通过ss -s 观察TCP连接数，发现实际建立连接的不多，而clodsed和timewait状态的连接很多
	内核中的连接跟踪模块，可能会导致timewait问题

+ 通过dmesg查看系统日志，如果有连接跟踪出了问题，应该会有nf_conntrack相关的日志。通过日志看到nf_conntrack: table full, dropping packet 的错误日志
+ 发现是连接跟踪所致，执行命令查看最大限制
$ sysctl net.netfilter.nf_conntrack_max
net.netfilter.nf_conntrack_max = 200
当前连接跟踪数
$ sysctl net.netfilter.nf_conntrack_count
net.netfilter.nf_conntrack_count = 200

+ 发现连接数只有200，显然太小了，增大链接
将连接跟踪限制增大到1048576
$ sysctl -w net.netfilter.nf_conntrack_max=1048576

+ 之后发现问题基本解决

之后通过测试发现，有大量的异常请求信息。然后 通过日志分析，进行查看后更新相应的配置信息

+ 之后为了进一步提升吞吐量， 通过netstat -s | grep socket 来抓取套接字的相关信息，是否有丢包现象
+ 可以看到有大量的套接字丢包现象，并且丢包都是套接字队列溢出导致的，通过ss -ltnp来分析套接字的队列大小
+ 可看到监听队列大小，需要增大 

```sh
查询nginx队列长度
cat /etc/nginx/nginx.conf | grep backlog

# somaxconn是系统级套接字监听队列上限
$ sysctl net.core.somaxconn
```
+ 查看系统配置的临时端口号范围

```sh

$ sysctl net.ipv4.ip_local_port_range
net.ipv4.ip_local_port_range=20000 20050

重新设置
$ sysctl -w net.ipv4.ip_local_port_range="10000 65535"
net.ipv4.ip_local_port_range = 10000 65535
```

+  大量连接处于timewait状态，而timewait状态本身会继续占用端口号，如果端口号可以重用，可以缩短检查过程，

```sh
查询端口号重用的配置
$ sysctl net.ipv4.tcp_tw_reuse
net.ipv4.tcp_tw_reuse = 0
0为禁止，1位开启。
```

